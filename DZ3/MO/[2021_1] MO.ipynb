{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семинар 7: \"Методы оптимизации\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ФИО:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romansim/miniforge3/envs/data-science/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/romansim/miniforge3/envs/data-science/lib/python3.9/site-packages/torchvision/image.so, 0x0006): symbol not found in flat namespace (__ZN3c106detail19maybe_wrap_dim_slowExxb)\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch import nn\n",
    "from torch.nn import NLLLoss, Sequential, Linear, Sigmoid, ELU, Tanh, L1Loss, Module, Parameter\n",
    "from torch.autograd import Variable\n",
    "from matplotlib.pyplot import cm\n",
    "from matplotlib.colors import LogNorm\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом семинаре мы попробуем сравнить различные методы оптимизации: GD, Momentum, NAG, Adagrad, Adadelta, Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1: Реализация методов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полезная функция: plt.contour\n",
    "Для всех экспериментов подберите параметры так, чтобы метод сошелся к ближайшему локальному минимуму. Все методы следует запускать из одной и той же точки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> 1.1 Реализуйте методы GD, Momentum, NAG, Adagrad, Adadelta, Adam.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, parameters):\n",
    "        self.parameters = list(parameters)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.parameters:\n",
    "            param.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "def optimize_function(fn, optim, optim_args, start_point, num_iter=50):\n",
    "    weigths = nn.Parameter(torch.FloatTensor(start_point), requires_grad=True)\n",
    "\n",
    "    optim = optim(parameters=[weigths], **optim_args)\n",
    "    points = []\n",
    "    losses = []\n",
    "    for i in range(num_iter):\n",
    "        if hasattr(optim, 'pre_step'):\n",
    "            optim.pre_step()\n",
    "        loss = fn(weigths[0], weigths[1])\n",
    "        points.append(weigths.data.detach().clone())\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    points = torch.stack(points, axis=0)\n",
    "    losses = torch.FloatTensor(losses)\n",
    "    return points, losses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compare_optimizers(\n",
    "    fn,\n",
    "    optim_list,\n",
    "    start_point,\n",
    "    x_range=(-5, 5),\n",
    "    y_range=(-5, 5),\n",
    "    xstep=0.2,\n",
    "    ystep=0.2,\n",
    "    minima=None,\n",
    "    num_iter = 50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Draw level lines with optimizer behaviour\n",
    "    \"\"\"\n",
    "    xmin, xmax = x_range\n",
    "    ymin, ymax = y_range\n",
    "    x, y = np.meshgrid(np.arange(xmin, xmax + xstep, xstep), np.arange(ymin, ymax + ystep, ystep))\n",
    "    z = fn(torch.from_numpy(x), torch.from_numpy(y))\n",
    "    z = z.detach().numpy()\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    ax.contour(x, y, z, levels=np.logspace(0, 5, 35), norm=LogNorm(), cmap=plt.cm.jet)\n",
    "    if minima:\n",
    "        ax.plot(*minima, 'r*', markersize=18)\n",
    "\n",
    "    fig.suptitle(\"Level lines of optimezed function\")\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "\n",
    "    ax.set_xlim((xmin, xmax))\n",
    "    ax.set_ylim((ymin, ymax))\n",
    "\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(optim_list)))\n",
    "    name_losses = {}\n",
    "\n",
    "    for c, (name, optim, args) in zip(colors, optim_list):\n",
    "        points, losses = optimize_function(fn, optim, args, start_point, num_iter)\n",
    "        ax.quiver(\n",
    "            points[:-1, 0], points[:-1, 1],\n",
    "            points[1:, 0] - points[:-1, 0], points[1:, 1] - points[:-1, 1],\n",
    "            scale_units='xy', angles='xy', scale=1, color=c,\n",
    "            label=name\n",
    "        )\n",
    "        name_losses[name] = losses\n",
    "\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(\"Loss behaviour in learning\")\n",
    "    plt.xlabel(\"Num of iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    x = np.arange(0, num_iter)\n",
    "    for name, losses in name_losses.items():\n",
    "        plt.plot(x, losses.numpy(), label=name)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, parameters, learning_rate=0.01):\n",
    "        super().__init__(parameters)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters:\n",
    "                param -= self.learning_rate * param.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Momentum(Optimizer):\n",
    "    def __init__(self, parameters, learning_rate=0.01, gamma=0.9):\n",
    "        super().__init__(parameters)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.v = [torch.zeros_like(param) for param in self.parameters]\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for v, param in zip(self.v, self.parameters):\n",
    "                v.copy_(self.gamma * v + self.learning_rate * param.grad)\n",
    "                param -= v"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NAG(Optimizer):\n",
    "    def __init__(self, parameters, learning_rate=0.01, gamma=0.9):\n",
    "        super().__init__(parameters)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.v = [torch.zeros_like(param) for param in self.parameters]\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for v, param in zip(self.v, self.parameters):\n",
    "                param -= v\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for v, param in zip(self.v, self.parameters):\n",
    "                param += v\n",
    "\n",
    "                v.copy_(self.gamma * v + self.learning_rate * param.grad)\n",
    "\n",
    "                param -= v"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp(Optimizer):\n",
    "    def __init__(self, parameters, learning_rate=0.01, gamma=0.9):\n",
    "        super().__init__(parameters)\n",
    "        \n",
    "        self.learning_rate = learning_rate * 10\n",
    "        \n",
    "        self.G = [torch.zeros_like(param) for param in self.parameters]\n",
    "        self.eps = 10e-9\n",
    "        \n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for g, param in zip(self.G, self.parameters):\n",
    "                g.copy_(self.gamma * g + (1 - self.gamma) * param.grad**2)\n",
    "                \n",
    "                param -= self.learning_rate / torch.sqrt(g + self.eps) * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AdaGrad(Optimizer):\n",
    "    def __init__(self, parameters, learning_rate=0.01):\n",
    "        super().__init__(parameters)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.eps = 1e-9\n",
    "        self.G = [torch.zeros_like(param) for param in self.parameters]\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for g, param in zip(self.G, self.parameters):\n",
    "                g.copy_(g + param.grad ** 2)\n",
    "                param -= self.learning_rate / (g + self.eps) ** 0.5 * param.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Adadelta(Optimizer):\n",
    "    def __init__(self, parameters, gamma=0.9):\n",
    "        super().__init__(parameters)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps = 10e-8\n",
    "        self.G = [torch.zeros_like(param) for param in self.parameters]\n",
    "        self.D_Theta = [torch.zeros_like(param) for param in self.parameters]\n",
    "\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for g, d_theta, param in zip(self.G, self.D_Theta, self.parameters):\n",
    "                g.copy_(self.gamma * g + (1 - self.gamma) * param.grad**2)\n",
    "                param_hat = (torch.sqrt(d_theta + self.eps) / torch.sqrt(g + self.eps)) * param.grad\n",
    "                d_theta.copy_(self.gamma * d_theta + (1 - self.gamma) * param_hat**2)\n",
    "                param -= param_hat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, parameters, learning_rate=0.01, beta_1=0.9, beta_2=0.99):\n",
    "        super().__init__(parameters)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.eps = 10e-9\n",
    "\n",
    "        self.M = [torch.zeros_like(param) for param in self.parameters]\n",
    "        self.V = [torch.zeros_like(param) for param in self.parameters]\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for m, v, param in zip(self.M, self.V, self.parameters):\n",
    "                m.copy_(self.beta_1 * m + (1 - self.beta_1) * param.grad)\n",
    "                v.copy_(self.beta_2 * v + (1 - self.beta_2) * param.grad**2)\n",
    "                m_hat = m / (1 - self.beta_1)\n",
    "                v_hat = v / (1 - self.beta_2)\n",
    "                param -= self.learning_rate / torch.sqrt(v_hat + self.eps) * m_hat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> 1.2 Сравните эти методы на функции $J(x, y) = x^2+y^2$</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def F1(x, y):\n",
    "    return x**2 + y**4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "compare_optimizers(\n",
    "    F1,\n",
    "    [\n",
    "        ('SGD', SGD, {}),\n",
    "        ('MOMENTUM SGD', Momentum, {\"gamma\": 0.9}),\n",
    "        ('NAG', NAG, {\"gamma\": 0.8}),\n",
    "        ('RMSProp', RMSProp, {\"gamma\": 0.9}),\n",
    "        ('AdaGrad', AdaGrad, {}),\n",
    "        ('Adadelta', Adadelta, {\"gamma\": 0.001}),\n",
    "        ('Adam', Adam, {\"learning_rate\": 0.035}),\n",
    "    ],\n",
    "    start_point=[3,3],\n",
    "    minima=(0,0),\n",
    "    num_iter=500,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>1.3 Сравните эти методы на функции $J(x, y) = x^2sin(x)+y^2sin(y)$</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def F2(x, y):\n",
    "    return (x ** 2) * torch.sin(x) + (y ** 2) * torch.sin(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_optimizers(\n",
    "    F2,\n",
    "    [\n",
    "        ('SGD', SGD, {}),\n",
    "        ('MOMENTUM SGD', Momentum, {\"gamma\": 0.9}),\n",
    "        ('NAG', NAG, {\"gamma\": 0.8}),\n",
    "        ('RMSProp', RMSProp, {\"gamma\": 0.9}),\n",
    "        ('AdaGrad', AdaGrad, {}),\n",
    "        ('Adadelta', Adadelta, {\"gamma\": 0.001}),\n",
    "        ('Adam', Adam, {\"learning_rate\": 0.035}),\n",
    "    ],\n",
    "    start_point=[2,1],\n",
    "    minima=(0,0),\n",
    "    num_iter=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>1.3 Сравните эти методы на функции $J(x,y)=x^2sin(x^2)+y^2sin(y^2)$</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def F3(x, y):\n",
    "    return (x ** 2) * torch.sin(x ** 2) + (y ** 2) * torch.sin(y ** 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_optimizers(\n",
    "    F3,\n",
    "    [\n",
    "        ('SGD', SGD, {}),\n",
    "        ('MOMENTUM SGD', Momentum, {\"gamma\": 0.9}),\n",
    "        ('NAG', NAG, {\"gamma\": 0.8}),\n",
    "        ('RMSProp', RMSProp, {\"gamma\": 0.9}),\n",
    "        ('AdaGrad', AdaGrad, {}),\n",
    "        ('Adadelta', Adadelta, {\"gamma\": 0.001}),\n",
    "        ('Adam', Adam, {\"learning_rate\": 0.035}),\n",
    "    ],\n",
    "    start_point=[-1,1],\n",
    "    minima=(0,0),\n",
    "    num_iter=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Часть 2: Обучение нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> 2.1 Сравните графики обучения для полносвязной нейросети на методах Adam, Adagrad, AdaDelta и SGD (на MNIST). Для обучения используйте оптимизаторы из первой части, а не из pytorch. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "!tar -zxvf MNIST.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('.', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('.', train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_net_learning_process(net, optimizer, train_loader, test_loader,\n",
    "                             epochs, criterion,\n",
    "                             plot=True, verbose=True, conv=False\n",
    "):\n",
    "\n",
    "    train_loss_epochs = []\n",
    "    test_loss_epochs = []\n",
    "    train_accuracy_epochs = []\n",
    "    test_accuracy_epochs = []\n",
    "    try:\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            losses = []\n",
    "            num_of_correct_pred = 0\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = Variable(data), Variable(target)\n",
    "                if not conv:\n",
    "                    data = data.view(-1, 28*28) # изменим размер с (batch_size, 1, 28, 28) на (batch_size, 28*28)\n",
    "                optimizer.zero_grad()\n",
    "                net_out = net(data)\n",
    "                loss = criterion(net_out, target)\n",
    "                losses.append(loss.data.item())\n",
    "\n",
    "                pred = net_out.data.max(1)[1]\n",
    "                num_of_correct_pred += pred.eq(target.data).sum().item()\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "            train_loss_epochs.append(np.mean(losses))\n",
    "            train_accuracy_epochs.append(num_of_correct_pred / len(train_loader.dataset))\n",
    "\n",
    "            losses = []\n",
    "            num_of_correct_pred = 0\n",
    "            for data, target in test_loader:\n",
    "                data, target = Variable(data), Variable(target)\n",
    "                if not conv:\n",
    "                    data = data.view(-1, 28 * 28)\n",
    "                net_out = net(data)\n",
    "\n",
    "                loss = criterion(net_out, target)\n",
    "                losses.append(loss.data.item())\n",
    "\n",
    "                # sum up batch loss\n",
    "                pred = net_out.data.max(1)[1]\n",
    "                num_of_correct_pred += pred.eq(target.data).sum().item()\n",
    "\n",
    "            test_loss_epochs.append(np.mean(losses))\n",
    "            test_accuracy_epochs.append(num_of_correct_pred / len(test_loader.dataset))\n",
    "\n",
    "            clear_output(wait=True) # для динамического обновления графиков, wait - очищает вывод\n",
    "\n",
    "            if verbose: # детализация выводимой информации\n",
    "                print(\n",
    "                      f'Network: <{type(net).__name__}>\\n'\n",
    "                      f'Optimizer: <{type(optimizer).__name__}>\\n'\n",
    "                      f'Loss type: <{type(criterion).__name__}>\\n\\n'\n",
    "                      f'Epoch: {epoch+1}\\n'\n",
    "                      f'<Train/Test>\\n'\n",
    "                      f'Loss: {np.round(train_loss_epochs[-1], 3)}/{np.round(test_loss_epochs[-1], 3)} '\n",
    "                      f'| Accuracy: {np.round(train_accuracy_epochs[-1], 3)}/{np.round(test_accuracy_epochs[-1], 3)}'\n",
    "                     )\n",
    "\n",
    "            if plot:\n",
    "                plt.figure(figsize=(12, 5))\n",
    "\n",
    "                # Отображение изменения ошибки\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.plot(train_loss_epochs, label='Train')\n",
    "                plt.plot(test_loss_epochs, label='Test')\n",
    "                plt.xlabel('Epochs', fontsize=16)\n",
    "                plt.ylabel('Loss', fontsize=16)\n",
    "                plt.legend(loc=0, fontsize=16)\n",
    "                plt.grid('on')\n",
    "\n",
    "                # Отображение изменения accuracy\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.plot(train_accuracy_epochs, label='Train')\n",
    "                plt.plot(test_accuracy_epochs, label='Test')\n",
    "                plt.xlabel('Epochs', fontsize=16)\n",
    "                plt.ylabel('Accuracy', fontsize=16)\n",
    "                plt.legend(loc=0, fontsize=16)\n",
    "                plt.grid('on')\n",
    "                plt.show()\n",
    "    except KeyboardInterrupt as KI:\n",
    "        print(KI)\n",
    "\n",
    "    return train_loss_epochs, \\\n",
    "       test_loss_epochs, \\\n",
    "       train_accuracy_epochs, \\\n",
    "       test_accuracy_epochs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compare_activation_func(loss_results: list, acc_results: list, labels: list) -> None:\n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for loss_result, label in zip(loss_results, labels):\n",
    "        plt.plot(loss_result, label=label)\n",
    "\n",
    "    plt.xlabel('Epochs', fontsize=16)\n",
    "    plt.ylabel('Loss', fontsize=16)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for acc_result, label in zip(acc_results, labels):\n",
    "        plt.plot(acc_result, label=label)\n",
    "\n",
    "    plt.xlabel('Epochs', fontsize=16)\n",
    "    plt.ylabel('Accuracy', fontsize=16)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NUM_EPOCHS=10\n",
    "nets = [FullyConnectedNN() for i in range(7)]\n",
    "print(nets[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizers = [\n",
    "                SGD(parameters=nets[0].parameters(), learning_rate=0.01),\n",
    "                Momentum(parameters=nets[1].parameters(), learning_rate=0.01, gamma=0.9),\n",
    "                NAG(parameters=nets[2].parameters(), learning_rate=0.01, gamma=0.9),\n",
    "                RMSProp(parameters=nets[3].parameters(), learning_rate=0.01, gamma=0.9),\n",
    "                Adadelta(parameters=nets[4].parameters(), gamma=0.9),\n",
    "                AdaGrad(parameters=nets[5].parameters(), learning_rate=0.01),\n",
    "                Adam(parameters=nets[6].parameters(), learning_rate=0.005, beta_1=0.9, beta_2=0.99),\n",
    "            ]\n",
    "criterion = nn.NLLLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_sgd, ts_sgd, tr_ac_sgd, ts_ac_sgd =\\\n",
    "run_net_learning_process(net=nets[0], optimizer=optimizers[0], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader,\n",
    "                         plot=True, verbose=True,\n",
    "                        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_mom, ts_mom, tr_ac_mom, ts_ac_mom =\\\n",
    "run_net_learning_process(net=nets[1], optimizer=optimizers[1], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader,\n",
    "                         plot=True, verbose=True,\n",
    "                        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_nag, ts_nag, tr_ac_nag, ts_ac_nag =\\\n",
    "run_net_learning_process(net=nets[2], optimizer=optimizers[2], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader,\n",
    "                         plot=True, verbose=True,\n",
    "                        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_rms, ts_rms, tr_ac_rms, ts_ac_rms =\\\n",
    "run_net_learning_process(net=nets[3], optimizer=optimizers[3], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader,\n",
    "                         plot=True, verbose=True,\n",
    "                        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_add, ts_add, tr_ac_add, ts_ac_add =\\\n",
    "run_net_learning_process(net=nets[4], optimizer=optimizers[4], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader,\n",
    "                         plot=True, verbose=True,\n",
    "                        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_adg, ts_adg, tr_ac_adg, ts_ac_adg =\\\n",
    "run_net_learning_process(net=nets[5], optimizer=optimizers[5], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader,\n",
    "                         plot=True, verbose=True,\n",
    "                        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_adm, ts_adm, tr_ac_adm, ts_ac_adm =\\\n",
    "run_net_learning_process(net=nets[6], optimizer=optimizers[6], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader,\n",
    "                         plot=True, verbose=True,\n",
    "                        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "compare_activation_func(loss_results=[ts_sgd, ts_mom, ts_nag, ts_rms, ts_add, ts_adg, ts_adm],\n",
    "                        acc_results=[ts_ac_sgd, ts_ac_mom, ts_ac_nag, ts_ac_rms, ts_ac_add, ts_ac_adg, ts_ac_adm],\n",
    "                        labels=[\"SGD\", \"Momentum SGD\", \"NAG\", \"RMSProp\", \"Adadelta\", \"AdaGrad\", \"Adam\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> 2.2 Сравните графики обучения для сверточной нейросети на методах Adam, Adagrad, AdaDelta и SGD. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 28\n",
    "CHANNELS = 1\n",
    "class ConvClassifier(nn.Module):\n",
    "    def __init__(self, image_size):\n",
    "        super(ConvClassifier, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(nn.Conv2d(CHANNELS, 3, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2))\n",
    "        self.linear_layers = nn.Sequential(nn.Linear(image_size//2*image_size//2*3, 10), nn.LogSoftmax(dim=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NUM_EPOCHS=10\n",
    "cnn_nets = [ConvClassifier(image_size=IMAGE_SIZE) for i in range(7)]\n",
    "print(cnn_nets[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizers = [\n",
    "                SGD(parameters=cnn_nets[0].parameters(), learning_rate=0.01),\n",
    "                Momentum(parameters=cnn_nets[1].parameters(), learning_rate=0.01, gamma=0.9),\n",
    "                NAG(parameters=cnn_nets[2].parameters(), learning_rate=0.01, gamma=0.9),\n",
    "                RMSProp(parameters=cnn_nets[3].parameters(), learning_rate=0.01, gamma=0.9),\n",
    "                Adadelta(parameters=cnn_nets[4].parameters(), gamma=0.9),\n",
    "                AdaGrad(parameters=cnn_nets[5].parameters(), learning_rate=0.01),\n",
    "                Adam(parameters=cnn_nets[6].parameters(), learning_rate=0.005, beta_1=0.9, beta_2=0.99),\n",
    "            ]\n",
    "criterion = nn.NLLLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_sgd, ts_sgd, tr_ac_sgd, ts_ac_sgd =\\\n",
    "run_net_learning_process(net=cnn_nets[0], optimizer=optimizers[0], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader,\n",
    "                         plot=True, verbose=True, conv=True,\n",
    "                        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_mom, ts_mom, tr_ac_mom, ts_ac_mom =\\\n",
    "run_net_learning_process(net=cnn_nets[1], optimizer=optimizers[1], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader,\n",
    "                         plot=True, verbose=True, conv=True,\n",
    "                        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_nag, ts_nag, tr_ac_nag, ts_ac_nag =\\\n",
    "run_net_learning_process(net=cnn_nets[2], optimizer=optimizers[2], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader,\n",
    "                         plot=True, verbose=True, conv=True,\n",
    "                        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_rms, ts_rms, tr_ac_rms, ts_ac_rms =\\\n",
    "run_net_learning_process(net=cnn_nets[3], optimizer=optimizers[3], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader,\n",
    "                         plot=True, verbose=True, conv=True,\n",
    "                        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_add, ts_add, tr_ac_add, ts_ac_add =\\\n",
    "run_net_learning_process(net=cnn_nets[4], optimizer=optimizers[4], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader,\n",
    "                         plot=True, verbose=True, conv=True,\n",
    "                        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_adg, ts_adg, tr_ac_adg, ts_ac_adg =\\\n",
    "run_net_learning_process(net=cnn_nets[5], optimizer=optimizers[5], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader,\n",
    "                         plot=True, verbose=True, conv=True,\n",
    "                        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_adm, ts_adm, tr_ac_adm, ts_ac_adm =\\\n",
    "run_net_learning_process(net=cnn_nets[6], optimizer=optimizers[6], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader,\n",
    "                         plot=True, verbose=True, conv=True,\n",
    "                        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "compare_activation_func(loss_results=[ts_sgd, ts_mom, ts_nag, ts_rms, ts_add, ts_adg, ts_adm],\n",
    "                        acc_results=[ts_ac_sgd, ts_ac_mom, ts_ac_nag, ts_ac_rms, ts_ac_add, ts_ac_adg, ts_ac_adm],\n",
    "                        labels=[\"SGD\", \"Momentum SGD\", \"NAG\", \"RMSProp\", \"Adadelta\", \"AdaGrad\", \"Adam\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedback (опционально)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь вы можете оставить список опечаток из лекции или семинара:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь вы можете оставить комментарии по лекции или семинару:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}